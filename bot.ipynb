{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bpn8Q6aTqDk3",
    "outputId": "8800f22d-aa48-4c6d-e3ff-408b42c12e5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyTelegramBotAPI\n",
      "  Downloading pytelegrambotapi-4.20.0-py3-none-any.whl (248 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.1/248.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pyTelegramBotAPI) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pyTelegramBotAPI) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pyTelegramBotAPI) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pyTelegramBotAPI) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pyTelegramBotAPI) (2024.6.2)\n",
      "Installing collected packages: pyTelegramBotAPI\n",
      "Successfully installed pyTelegramBotAPI-4.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyTelegramBotAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "referenced_widgets": [
      "53e0d27086504b1bb0db1219656911c2",
      "bea90c31cdf141a1b9c194b9efadff51",
      "a5acb776faea49f78b50ff324eae115c",
      "205160e6865d4425aa7ff99a3a80afc6",
      "3abd8db728d64fa395b7993ec55f1135",
      "0724ae0960564df5b9215bd1f21bf402",
      "61ac5f6d5d7342e0b8b26b6cd6cd7b85"
     ]
    },
    "id": "E_wrQ5jOvu4P",
    "outputId": "af19c27e-62d3-4153-e327-71018628a781"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e0d27086504b1bb0db1219656911c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/251 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea90c31cdf141a1b9c194b9efadff51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5acb776faea49f78b50ff324eae115c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205160e6865d4425aa7ff99a3a80afc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abd8db728d64fa395b7993ec55f1135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0724ae0960564df5b9215bd1f21bf402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/136k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ac5f6d5d7342e0b8b26b6cd6cd7b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/470M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import telebot\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering, pipeline\n",
    "import google.generativeai as genai\n",
    "import base64\n",
    "\n",
    "# Replace with your Telegram bot token and Hugging Face API key\n",
    "TELEGRAM_TOKEN = \"7995379796:AAF_Imy8fPlh-gWUNXkckY5nexttTlSdpGA\"\n",
    "HUGGING_API_KEY = \"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\"\n",
    "GEMINI_API_KEY = \"AIzaSyCdjv4z6_N95XfwUCzWzTWS7q2FUAi6nOY\"\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/magorshunov/layoutlm-invoices\"\n",
    "headers = {\"Authorization\": f\"Bearer {HUGGING_API_KEY}\"}\n",
    "\n",
    "# Initialize the Telegram bot\n",
    "bot = telebot.TeleBot(TELEGRAM_TOKEN)\n",
    "\n",
    "# Initialize the VQA model and processor\n",
    "vqa_processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "vqa_model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "# Configure Google Gemini API\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Function to call Google Gemini API for chatbot responses\n",
    "def call_gemini_api(prompt):\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "# Function to answer visual questions based on an image\n",
    "def answer_question(image, question):\n",
    "    encoding = vqa_processor(image, question, return_tensors=\"pt\")\n",
    "    outputs = vqa_model(**encoding)\n",
    "    logits = outputs.logits\n",
    "    idx = logits.argmax(-1).item()\n",
    "    return vqa_model.config.id2label[idx]\n",
    "\n",
    "# Function to answer document questions using Hugging Face API\n",
    "def answer_document_question(image_path, question):\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        img = f.read()\n",
    "    img_base64 = base64.b64encode(img).decode(\"utf-8\")\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": {\n",
    "            \"image\": img_base64,\n",
    "            \"question\": question\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    result = response.json()\n",
    "\n",
    "    # Check if the result is a list and handle it accordingly\n",
    "    if isinstance(result, list):\n",
    "        if result:  # Check if the list is not empty\n",
    "            return result[0].get(\"answer\", \"No answer found.\")  # Assuming the answer is in the first element\n",
    "        else:\n",
    "            return \"No answer found.\"\n",
    "    else:\n",
    "        return result.get(\"answer\", \"No answer found.\")  # Handle the case where result is a dictionary\n",
    "\n",
    "# Bot handlers\n",
    "@bot.message_handler(commands=['start', 'help'])\n",
    "def send_welcome(message):\n",
    "    markup = telebot.types.ReplyKeyboardMarkup(row_width=2, resize_keyboard=True)\n",
    "    vqa_btn = telebot.types.KeyboardButton('Visual Question Answering')\n",
    "    doc_qa_btn = telebot.types.KeyboardButton('Document Question Answering')\n",
    "    chatbot_btn = telebot.types.KeyboardButton('Chatbot')\n",
    "    markup.add(vqa_btn, doc_qa_btn, chatbot_btn)\n",
    "    bot.send_message(message.chat.id, \"Choose an option:\", reply_markup=markup)\n",
    "\n",
    "@bot.message_handler(func=lambda message: message.text == 'Visual Question Answering')\n",
    "def vqa_option(message):\n",
    "    msg = bot.send_message(message.chat.id, \"Upload an image and ask a question:\")\n",
    "    bot.register_next_step_handler(msg, process_vqa)\n",
    "\n",
    "def process_vqa(message):\n",
    "    if message.photo:\n",
    "        file_info = bot.get_file(message.photo[-1].file_id)\n",
    "        downloaded_file = bot.download_file(file_info.file_path)\n",
    "        with open(\"image.jpg\", 'wb') as new_file:\n",
    "            new_file.write(downloaded_file)\n",
    "        img = Image.open(\"image.jpg\")\n",
    "        msg = bot.send_message(message.chat.id, \"Ask a question about the image:\")\n",
    "        bot.register_next_step_handler(msg, get_vqa_answer, img)\n",
    "    else:\n",
    "        bot.send_message(message.chat.id, \"Please upload an image.\")\n",
    "\n",
    "def get_vqa_answer(message, img):\n",
    "    question = message.text\n",
    "    answer = answer_question(img, question)\n",
    "    bot.send_message(message.chat.id, answer)\n",
    "\n",
    "@bot.message_handler(func=lambda message: message.text == 'Document Question Answering')\n",
    "def doc_qa_option(message):\n",
    "    msg = bot.send_message(message.chat.id, \"Upload an image of the document and ask a question:\")\n",
    "    bot.register_next_step_handler(msg, process_doc_qa)\n",
    "\n",
    "def process_doc_qa(message):\n",
    "    if message.photo:\n",
    "        file_info = bot.get_file(message.photo[-1].file_id)\n",
    "        downloaded_file = bot.download_file(file_info.file_path)\n",
    "        with open(\"document.jpg\", 'wb') as new_file:\n",
    "            new_file.write(downloaded_file)\n",
    "        msg = bot.send_message(message.chat.id, \"Ask a question about the document:\")\n",
    "        bot.register_next_step_handler(msg, get_doc_qa_answer, \"document.jpg\")\n",
    "    else:\n",
    "        bot.send_message(message.chat.id, \"Please upload an image of the document.\")\n",
    "\n",
    "def get_doc_qa_answer(message, image_path):\n",
    "    question = message.text\n",
    "    answer = answer_document_question(image_path, question)\n",
    "    bot.send_message(message.chat.id, answer)\n",
    "\n",
    "@bot.message_handler(func=lambda message: message.text == 'Chatbot')\n",
    "def chatbot_option(message):\n",
    "    msg = bot.send_message(message.chat.id, \"Ask me anything:\")\n",
    "    bot.register_next_step_handler(msg, chatbot_response)\n",
    "\n",
    "def chatbot_response(message):\n",
    "    session_id = message.chat.id\n",
    "    user_input = message.text\n",
    "    response = call_gemini_api(user_input)  # Using Gemini API for chatbot response\n",
    "    bot.send_message(message.chat.id, response)\n",
    "\n",
    "# Start the bot\n",
    "bot.polling()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
